{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db4450d",
   "metadata": {},
   "source": [
    "## Mahyar Mohammadi Matin - 610398166 -MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ef95c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import random\n",
    "class Mlp:\n",
    "    w = []\n",
    "    derv = []\n",
    "    neron = []\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        self.w = []\n",
    "        self.derv = []\n",
    "        for i in range(len(layers)-1):\n",
    "            self.derv.append(np.zeros((layers[i],layers[i+1])))\n",
    "            self.w.append(np.random.rand(layers[i],layers[i+1]))\n",
    "            \n",
    "        self.neron = []\n",
    "        for i in range(len(layers)):self.neron.append(np.array([0]*layers[i]))\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_derv(self, x):\n",
    "        return x * (1.0 - x)    \n",
    "        \n",
    "    def forward(self, inp):\n",
    "        val = np.array(inp)\n",
    "        self.neron[0] = val\n",
    "        for i,w in enumerate(self.w):\n",
    "            val = self.sigmoid(np.dot(val, w))\n",
    "            self.neron[i+1] = val\n",
    "        return self.neron[-1]\n",
    "            \n",
    "\n",
    "    def training(self, inputs, targets, alpha,repeat):\n",
    "        for i in range(repeat):\n",
    "            accuracy = 0\n",
    "            j=0\n",
    "            for inp in inputs:\n",
    "                output = self.forward(inp)\n",
    "                \n",
    "                #back\n",
    "                error = targets[j] - output\n",
    "                if(output<0.5):output=0\n",
    "                else:output=1\n",
    "                if(targets[j] == output):\n",
    "                    accuracy += 1 \n",
    "                \n",
    "                for k in list(range(len(self.derv)))[::-1]:\n",
    "                    delta = error * self.sigmoid_derv(self.neron[k+1])\n",
    "                    delta2 = delta.reshape(delta.shape[0], -1).T\n",
    "                    value = self.neron[k].reshape(self.neron[k].shape[0],-1)\n",
    "                    self.derv[k] = np.dot(value, delta2)\n",
    "                    error = np.dot(delta, self.w[k].T)\n",
    "                    \n",
    "                for k in range(len(self.w)):\n",
    "                    self.w[k] += self.derv[k] * alpha\n",
    "                    \n",
    "                j+=1\n",
    "            \n",
    "            if((i+1)%5==0):\n",
    "                print(f\"{i+1}.Accuracy: {accuracy*100/len(inputs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b636b70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.Accuracy: 92.25\n",
      "10.Accuracy: 92.025\n",
      "15.Accuracy: 91.7\n",
      "20.Accuracy: 92.075\n",
      "25.Accuracy: 92.1\n",
      "30.Accuracy: 92.4\n",
      "35.Accuracy: 92.3\n",
      "40.Accuracy: 92.3\n",
      "45.Accuracy: 92.325\n",
      "50.Accuracy: 92.325\n",
      "55.Accuracy: 92.425\n",
      "60.Accuracy: 92.375\n",
      "65.Accuracy: 92.375\n",
      "70.Accuracy: 92.425\n",
      "75.Accuracy: 92.425\n",
      "80.Accuracy: 92.45\n",
      "85.Accuracy: 92.475\n",
      "90.Accuracy: 92.525\n",
      "95.Accuracy: 92.5\n",
      "100.Accuracy: 92.45\n"
     ]
    }
   ],
   "source": [
    "#first let's make our mlp\n",
    "mlp = Mlp([2,15,20,1])\n",
    "\n",
    "#now read data to train\n",
    "file = open('train')\n",
    "inp = file.read()\n",
    "file.close()\n",
    "inp = inp.split('\\n')\n",
    "inp = [item.split('\\t')for item in inp]\n",
    "inp= inp[:-1]\n",
    "first_layer = np.array([[float(item[0]),float(item[1])]for item in inp])\n",
    "second_layer = np.array([int(item[2]) for item in inp])\n",
    "\n",
    "mlp.training(first_layer, second_layer, 1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "ca1b8554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp weights:\n",
      "[[ 5.67093068  4.07192497  3.58622525  4.95679144 -3.40102532  0.45975598\n",
      "  -5.01574354  2.41525363 -7.64770412  5.45330599 -4.07069768  3.90233896\n",
      "   6.6328934   0.14345208 -0.00908505]\n",
      " [ 2.25339762 -1.00549559  1.73358761  1.99635268  0.85477151  4.23364712\n",
      "  -2.35727228  1.77525254 -2.18997488  2.19442672 -2.53882044  1.87170876\n",
      "   2.53829574  3.87641929  3.65382131]]\n",
      "[[ 1.27010432e+00 -5.66191740e-02 -6.82977222e-01  3.02025230e-01\n",
      "   1.18653699e+00 -7.56049663e-01  1.65247841e-01 -3.84036666e-01\n",
      "  -2.18018613e-01  5.50383074e-01 -5.77288963e-01 -4.33420764e-01\n",
      "  -1.21375349e+00  7.05688017e-01 -3.19918829e-01 -2.86842523e-01\n",
      "  -9.87829292e-01 -8.44098167e-01 -7.25031033e-01 -1.22898605e+00]\n",
      " [-2.62048171e+00 -6.17382606e-01 -8.82282978e-01 -2.35164076e-01\n",
      "  -3.03259561e+00 -1.57925976e+00  3.05156048e-01 -1.52979005e+00\n",
      "  -3.09674772e-01  8.92848413e-02 -3.81155578e-01 -1.04934974e+00\n",
      "  -1.57758930e+00  1.17274328e+00 -1.28534421e+00 -8.74339983e-01\n",
      "  -2.79432470e+00 -1.32298333e+00 -1.03721419e+00 -1.82408213e+00]\n",
      " [ 1.11034409e-01 -1.62523172e-01 -3.81855444e-01  2.13970172e-02\n",
      "   8.06693050e-01 -8.69519973e-01 -1.81723876e-01 -6.94344800e-01\n",
      "   1.97773356e-02 -1.02024072e-01  2.34270060e-01 -7.39872782e-01\n",
      "  -7.28800470e-01  2.98752439e-01 -7.57107346e-01 -5.46635434e-01\n",
      "  -7.35961217e-01 -7.71723406e-01 -6.23065849e-01 -9.89548235e-01]\n",
      " [ 9.49801934e-01 -4.28976888e-01 -8.99867039e-02  5.27768018e-01\n",
      "   1.20917686e+00 -6.53510241e-01 -3.58323165e-01 -9.78896769e-01\n",
      "   5.23087650e-02  3.06940195e-02  4.32252202e-02 -1.47384903e-01\n",
      "  -4.25295229e-01  6.37183741e-01 -1.39835829e-01 -4.56947352e-01\n",
      "  -5.99878380e-01 -1.11009547e+00 -9.52676338e-01 -1.19830175e+00]\n",
      " [-1.70713235e+00 -1.73412726e+00  3.18434226e-01 -2.78882496e+00\n",
      "  -1.31352721e+00  1.37325738e+00 -4.66790367e+00  5.91501829e-01\n",
      "  -2.15379596e+00 -2.69226897e+00 -1.30537599e+00  7.87743171e-01\n",
      "   3.22869774e-01 -3.78206211e+00  3.01522724e-01  1.16419621e+00\n",
      "   2.47635667e+00  3.01380940e-01  1.94539953e-01  8.57341424e-01]\n",
      " [-4.07319566e-01 -4.06777248e-01  8.37701792e-03 -2.02033712e+00\n",
      "   6.98398123e-01 -1.67643127e+00 -1.65528697e+00  9.94204691e-01\n",
      "  -5.78026962e-01 -1.76565448e+00 -9.02188051e-01  9.69998995e-01\n",
      "   1.90501150e+00 -2.37249581e+00 -4.24020522e-02 -7.42018108e-01\n",
      "  -3.71415630e+00  1.47330830e+00  4.00496287e-01  1.58119619e+00]\n",
      " [-5.81510111e+00 -9.03171344e-01 -1.14446787e+00 -1.67103056e+00\n",
      "  -6.92188810e+00 -8.81444200e-01 -1.23520917e+00 -9.71560930e-01\n",
      "  -7.26205435e-01 -1.49663690e+00 -1.25345360e+00 -1.54501522e+00\n",
      "  -1.19498942e+00 -1.22920800e+00 -4.82605210e-01 -1.09499171e+00\n",
      "   7.21039796e-02 -8.39206178e-01 -1.59089143e+00 -1.28748028e+00]\n",
      " [-6.77868615e-02 -1.37493404e-02 -9.40207003e-01  1.45980841e-01\n",
      "  -1.48007067e-01 -1.37778785e+00  5.38710648e-01 -3.96345416e-01\n",
      "  -5.58564022e-02  1.97863215e-01 -2.46531033e-01 -6.92731868e-01\n",
      "  -6.94951849e-01 -1.56951636e-01 -3.22769388e-01 -5.95439524e-01\n",
      "  -7.13628292e-01 -3.41133299e-01 -5.27303964e-01 -5.97737756e-01]\n",
      " [-3.18136141e+00 -1.53045831e+00 -4.20391528e-01 -1.63770152e+00\n",
      "  -4.26883964e+00 -4.07519459e-01 -1.82882548e+00 -6.29246317e-01\n",
      "  -1.59534661e+00 -1.03988434e+00 -1.63568621e+00 -7.33160288e-02\n",
      "  -9.83751267e-02 -2.63918068e+00 -5.42927492e-01  7.00565618e-02\n",
      "  -7.46777843e-01 -4.77534371e-01  1.02732731e-02 -5.93104557e-01]\n",
      " [ 8.14381338e-01  1.95783505e-02 -6.87842897e-01  1.76194941e-01\n",
      "   1.09625899e+00 -5.77341055e-01  3.36280853e-01 -1.03422750e+00\n",
      "   2.42497301e-01  8.66039182e-02  5.53290036e-03 -9.47342343e-01\n",
      "  -6.51616618e-01  1.06405658e+00 -5.02697951e-01 -1.07679775e+00\n",
      "  -2.24806879e-01 -6.08322832e-01 -1.93439224e-01 -5.83773853e-01]\n",
      " [-5.72763979e+00 -1.14509958e+00 -9.04343732e-01 -9.91559290e-01\n",
      "  -6.99383916e+00 -3.88141236e-01 -9.71320084e-01 -9.30409683e-01\n",
      "  -1.52772074e+00 -1.77454298e+00 -1.05667547e+00 -1.35330691e+00\n",
      "  -2.39652070e+00 -1.36711496e+00 -1.06160391e+00 -1.36375684e+00\n",
      "   2.41398833e-01 -1.64331811e+00 -1.03724427e+00 -1.35608327e+00]\n",
      " [ 1.35915171e-01 -5.66853359e-01 -3.82328747e-02 -1.47544465e-01\n",
      "   7.82437320e-01 -5.58985293e-01  4.87312115e-01 -1.26531439e-01\n",
      "  -3.41772460e-01 -3.20204874e-01 -1.27349834e-01 -1.09807261e+00\n",
      "  -4.89010708e-01  9.10285363e-01 -8.59427116e-01 -4.99444436e-01\n",
      "  -1.05702239e+00 -8.18999173e-01 -7.75269264e-01 -9.58838262e-01]\n",
      " [ 1.08788580e+00  3.43145601e-01 -8.05272232e-01  5.60202386e-01\n",
      "   1.39091975e+00 -5.70185789e-01  3.58568287e-01 -4.34432348e-01\n",
      "  -1.28148615e-02  1.76339039e-01 -7.41081448e-02 -8.64852171e-01\n",
      "  -1.25224143e+00  1.18209816e+00 -6.54357929e-01 -5.05802759e-01\n",
      "  -7.95153251e-01 -1.16583978e+00 -6.24446042e-01 -7.60720150e-01]\n",
      " [ 1.63741588e-01 -8.26960890e-01 -3.85917604e-01 -1.51026620e+00\n",
      "   5.64663041e-01 -1.39142512e+00 -5.36756096e-01  1.32905127e-01\n",
      "  -7.39685211e-01 -9.53376439e-01 -8.32980875e-01  4.71057635e-01\n",
      "   1.67954688e+00 -2.19542617e+00  7.61130765e-02 -9.98107439e-01\n",
      "  -3.98204499e+00  9.04787877e-01  7.65748353e-01  1.03022126e+00]\n",
      " [-3.01651970e-01 -6.65912081e-01 -6.60748194e-02 -1.17926313e+00\n",
      "   5.90523542e-01 -1.56302400e+00 -1.60564551e+00 -3.57582801e-01\n",
      "  -1.33360606e+00 -1.17064814e+00 -6.66069305e-01  8.98231014e-02\n",
      "   1.77046467e+00 -2.33511969e+00 -6.28992950e-01 -1.08089575e+00\n",
      "  -3.64531687e+00  6.20880054e-01  3.03939423e-01  8.48612630e-01]]\n",
      "[[ 2.83575821]\n",
      " [ 0.33120943]\n",
      " [-1.02486452]\n",
      " [ 0.73698089]\n",
      " [ 2.94786469]\n",
      " [-2.00023589]\n",
      " [ 0.88278393]\n",
      " [-1.18693164]\n",
      " [ 0.63094504]\n",
      " [ 0.86655898]\n",
      " [ 0.33185085]\n",
      " [-1.26499308]\n",
      " [-1.32313465]\n",
      " [ 1.03457373]\n",
      " [-0.92000884]\n",
      " [-1.32951852]\n",
      " [-3.52095359]\n",
      " [-1.47849702]\n",
      " [-1.13504303]\n",
      " [-1.53905863]]\n",
      "\n",
      "Total accuracy for test:  92.475\n"
     ]
    }
   ],
   "source": [
    "print(\"mlp weights:\")\n",
    "for item in mlp.w:\n",
    "    print(item)\n",
    "print()\n",
    "\n",
    "file = open('train')\n",
    "inp = file.read()\n",
    "file.close()\n",
    "inp = inp.split('\\n')\n",
    "inp = [item.split('\\t')for item in inp]\n",
    "inp= inp[:-1]\n",
    "first_layer = np.array([[float(item[0]),float(item[1])]for item in inp])\n",
    "second_layer = np.array([int(item[2]) for item in inp])\n",
    "accuracy=0\n",
    "for i in range(len(inp)):\n",
    "    test = np.array(first_layer[i])\n",
    "    result = np.array(second_layer[i])\n",
    "    output = mlp.forward(test)\n",
    "    if(output<0.5):output=0\n",
    "    else:output=1\n",
    "    if(result == output):\n",
    "        accuracy += 1 \n",
    "\n",
    "print(\"Total accuracy for test: \",accuracy*100/len(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc2324",
   "metadata": {},
   "source": [
    "alpha must be at 0.2 to 2 and the best alpha i found is 1\n",
    "for layers we can't add more than 2 hidden layers. also the best result is when we have 15-25 neurons in each hidden layer. and when we have equal neurons in each layer the result is improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
